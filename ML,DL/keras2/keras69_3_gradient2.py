# 최적의 weight를 찾기 위해
# learning rate를 점점 줄여본다.

import numpy as np
f = lambda x : x**2 - 4*x + 6

# f를 미분한 함수
gradient = lambda x : 2*x - 4

x0 = 100.0                  # 랜덤한 x값을 준다.
epoch = 100
learning_rate = 0.1
# learning_rate = 0.01      # learning rate를 줄이면 그만큼 epoch을 늘려야 한다.

print("step\tx\tf(x)")                                  # \t : tab
print("{:02d}\t{:6.5f}\t{:6.5f}".format(0, x0, f(x0)))  # .format : { } 에 값을 넣어주는 기능

for i in range(epoch) :
    temp = x0 - learning_rate * gradient(x0)            # 미분한 값과 learning rate를 곱해준고 원값에 빼준다.
    x0 = temp

    print("{:02d}\t{:6.5f}\t{:6.5f}".format(i+1, x0, f(x0)))

'''
step    x       f(x)
00      100.00000       9606.00000
01      80.40000        6148.56000
02      64.72000        3935.79840
03      52.17600        2519.63098
04      42.14080        1613.28382
05      34.11264        1033.22165
06      27.69011        661.98185 
07      22.55209        424.38839 
08      18.44167        272.32857 
09      15.15334        175.01028
10      12.52267        112.72658
11      10.41814        72.86501
12      8.73451 47.35361
13      7.38761 31.02631
14      6.31009 20.57684
15      5.44807 13.88918
16      4.75845 9.60907
17      4.20676 6.86981
18      3.76541 5.11668
19      3.41233 3.99467
20      3.12986 3.27659
21      2.90389 2.81702
22      2.72311 2.52289
23      2.57849 2.33465
24      2.46279 2.21418
25      2.37023 2.13707
26      2.29619 2.08773
27      2.23695 2.05615
28      2.18956 2.03593
29      2.15165 2.02300
30      2.12132 2.01472
31      2.09705 2.00942
32      2.07764 2.00603
33      2.06211 2.00386
34      2.04969 2.00247
35      2.03975 2.00158
36      2.03180 2.00101
37      2.02544 2.00065
38      2.02035 2.00041
39      2.01628 2.00027
40      2.01303 2.00017
41      2.01042 2.00011
42      2.00834 2.00007
43      2.00667 2.00004
44      2.00534 2.00003
45      2.00427 2.00002
46      2.00341 2.00001
47      2.00273 2.00001
48      2.00219 2.00000
49      2.00175 2.00000
50      2.00140 2.00000
51      2.00112 2.00000
52      2.00090 2.00000
53      2.00072 2.00000
54      2.00057 2.00000
55      2.00046 2.00000
56      2.00037 2.00000
57      2.00029 2.00000
58      2.00023 2.00000
59      2.00019 2.00000
60      2.00015 2.00000
61      2.00012 2.00000
62      2.00010 2.00000
63      2.00008 2.00000
64      2.00006 2.00000
65      2.00005 2.00000
66      2.00004 2.00000
67      2.00003 2.00000
68      2.00003 2.00000
69      2.00002 2.00000
70      2.00002 2.00000
71      2.00001 2.00000
72      2.00001 2.00000
73      2.00001 2.00000
74      2.00001 2.00000
75      2.00001 2.00000
76      2.00000 2.00000
77      2.00000 2.00000
78      2.00000 2.00000
79      2.00000 2.00000
80      2.00000 2.00000
81      2.00000 2.00000
82      2.00000 2.00000
83      2.00000 2.00000
84      2.00000 2.00000
85      2.00000 2.00000
86      2.00000 2.00000
87      2.00000 2.00000
88      2.00000 2.00000
89      2.00000 2.00000
90      2.00000 2.00000
91      2.00000 2.00000
92      2.00000 2.00000
93      2.00000 2.00000
94      2.00000 2.00000
95      2.00000 2.00000
96      2.00000 2.00000
97      2.00000 2.00000
98      2.00000 2.00000
99      2.00000 2.00000
100     2.00000 2.00000
'''