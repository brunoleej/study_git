import numpy as np
import torch

# 데이터에 대한 이해(Data Definition)
    # 학습할 데이터에 대해서 알아봅니다.

# 가설(Hypothesis) 수립
    # 가설을 수립하는 방법에 대해서 알아봅니다.

# 손실 계산하기(Compute loss)
    # 학습 데이터를 이용해서 연속적으로 모델을 개선시키는데 이 때 손실(loss)를 이용합니다.

# 경사 하강법(Gradient Descent)
    # 학습을 위한 핵심 알고리즘인 경사 하강법(Gradient Descent)에 대해서 이해합니다.

# 1. Data Definition
# 어떤 학생이 1시간 공부를 했더니 2점, 다른 학생이 2시간 공부를 했더니 4점, 또 다른 학생이 3시간을 공부했더니 6점을 맞았습니다. 
# 그렇다면, 내가 4시간을 공부한다면 몇 점을 맞을 수 있을까요?

# 이 질문에 대답하기 위해서 1시간, 2시간, 3시간을 공부했을 때 각각 2점, 4점, 6점이 나왔다는 앞서 나온 정보를 이용해야 합니다. 
# 이때 예측을 위해 사용하는 데이터를 훈련 데이터셋(training dataset)이라고 합니다. 
# 학습이 끝난 후, 이 모델이 얼마나 잘 작동하는지 판별하는 데이터셋을 테스트 데이터셋(test dataset)이라고 합니다.

# 2. 훈련 Dataset의 구성
# 앞서 텐서에 대해서 배웠는데, 모델을 학습시키기 위한 데이터는 파이토치의 텐서의 형태(torch.tensor)를 가지고 있어야 합니다.
# 그리고 입력과 출력을 각기 다른 텐서에 저장할 필요가 있습니다. 이때 보편적으로 입력은 x, 출력은 y를 사용하여 표기합니다.
# 여기서 x_train은 공부한 시간, y_train은 그에 맵핑되는 점수를 의미합니다.

x_train = torch.FloatTensor([[1],[2],[3]])
y_train = torch.FloatTensor([[2],[4],[6]])

# 이제 모델의 가설을 세워보겠습니다.

# 2. Hypothesis 수립
# 머신 러닝에서 식을 세울때 이 식을 가설(Hypothesis)라고 합니다. 
# 보통 머신 러닝에서 가설은 임의로 추측해서 세워보는 식일수도 있고, 경험적으로 알고 있는 식일 수도 있습니다. 
# 그리고 맞는 가설이 아니라고 판단되면 계속 수정해나가게 되는 식이기도 합니다.

# 선형 회귀의 가설은 이미 널리 알려져있으므로 고민할 필요가 없습니다. 선형 회귀란 학습 데이터와 가장 잘 맞는 하나의 직선을 찾는 일입니다. 
# 이때 선형 회귀의 가설(직선의 방정식)은 아래와 같은 형식을 가집니다.
# y = Wx + b
# 가설의 H를 따서  y대신 다음과 같이 식을 표현하기도 합니다.
# H(x) = Wx + b
# 이때 x와 곱해지는 W를 가중치(Weight)라고 하며, b를 편향(bias)이라고 합니다.

# 3. cost function에 대한 이해
# 비용 함수(cost function) = 손실 함수(loss function) = 오차 함수(error function) = 목적 함수(objective function)

# 4. Optimizer - Gradient Descent
# 이제 앞서 정의한 비용 함수(Cost Function)의 값을 최소로 하는 W와 b를 찾는 방법에 대해서 배울 차례입니다. 
# 이때 사용되는 것이 옵티마이저(Optimizer) 알고리즘입니다. 최적화 알고리즘이라고도 부릅니다. 
# 그리고 이 옵티마이저 알고리즘을 통해 적절한 W와 n를 찾아내는 과정을 머신 러닝에서 학습(training)이라고 부릅니다. 
# 여기서는 가장 기본적인 옵티마이저 알고리즘인 경사 하강법(Gradient Descent)에 대해서 배웁니다.

